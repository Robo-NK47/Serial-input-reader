import pickle
import re
import time
import serial as ser
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import threading
import os
import random
import statistics
import concurrent.futures
import itertools
import operator


def user_pref_input():
    # Makes a list containing the users preferences
    con = False
    while not con:
        userpref = []
        try:
            userpref.append(float(input("Please enter the number of samples you wish to record: ")))
            userpref.append(int(input('What is the number of sensors? ')))
            userpref.append(ser.Serial(str(input("What is the port you are using? ")), 57600))
            userpref.append(float(input("What is the maximal voltage? ")))
            userpref.append(int(input('How many recordings would you like to preform? ')))
            pose = ['Index finger', 'Fist', 'Open palm', 'Peace', 'Thumb up']
            print(' 0 = Index finger \n 1 = Fist \n 2 = Open palm \n 3 = Peace \n 4 = Thumb up')
            image = Image.open('Poses.jpg')
            # image.show()
            i = int(input("please select the recorded pose: "))
            userpref.append(pose[i])
            userpref.append(str(input("Enter your name: ")))
            con = True
        except:
            print("Invalid input")
    return userpref


def data_recorder(userpref, live_data):
    ard1 = userpref[2]
    norm_bul = False
    recordedData = []
    # This loop reads the raw data, the data type is 'byte'
    # The loops turns it into a float
    # The loop returns a nested list, every list inside the nested list
    # contains the sample number and the data
    while len(recordedData) < userpref[1]:
        while True:
            if ard1.inWaiting() > 0:
                current_data = str(ard1.readline())
                break
        while current_data.find('aaaa') == (-1) and (ard1.inWaiting() > 0):
            current_data = str(ard1.readline())
        i = 0
        while len(recordedData) < userpref[1]:
            if ard1.inWaiting() > 0:
                current_data = str(ard1.readline())

                try:
                    current_data = float(re.search(r'\d+', current_data).group())
                    current_sample = current_data / 1023
                    if current_sample > 1:
                        current_sample = live_data[i]

                except:
                    current_sample = live_data[i]
                recordedData.append(current_sample)
            i = i + 1
            if i > userpref[1]:
                i = 0
    if norm_bul:
        recordedData = vector_normalizer(recordedData)
        print(recordedData)
        return recordedData
    else:
        print(recordedData)
        return recordedData


def countdown():
    # Gives the user a few seconds to prepare for a
    # data recording session
    time.sleep(0.5)
    print(" ")
    print("The recording will begin in:")
    for i in range(0, 3):
        print(3 - i, "seconds")
        time.sleep(0.5)
    print(" ")


def training_dataset_maker(userpref):
    data_set_for_training = [data_recorder(userpref, [0, 0, 0, 0, 0, 0])]
    for i in range(userpref[0] - 1):
        data_set_for_training.append(data_recorder(userpref, data_set_for_training[i]))
    return data_set_for_training


def nested_list_flipper(nested):
    return_object = []
    for i in range(len(nested[0])):
        sensor = []
        for j in range(len(nested)):
            sensor.append(nested[j][i])
        return_object.append(sensor)
    return return_object


def file_opener(file_name):
    my_file = pickle.load(file=open(file_name, "rb"))
    return my_file


def file_delete(file_name):
    os.remove(file_name)


def file_saver(user_pref, suffix_num, data, file_type_num):
    file_type = ['.pkl', '.jpeg']
    suffix = [' - Training dataset file ', ' - Graph ']
    pose = ['Index finger ', 'Fist ', 'Open palm ', 'Peace ', 'Thumb up ']
    t = str(time.asctime())
    file_name = str(pose[user_pref[5]]) + str(re.sub('[:!@#$]', '_', t)) + suffix[suffix_num] \
                + file_type[file_type_num]
    with open(file_name, "wb") as f:
        pickle.dump(data, f)
        del data


def file_update(file_name, data):
    old_data = file_opener(file_name)
    file_delete(file_name)
    data = old_data + data
    with open(file_name, "wb") as f:
        pickle.dump(data, f)
        del data


def simple_graph_saver(data, user_pref, suffix_num):
    # Saves a simple X-Y graph for a given data set
    pose = ['Index finger ', 'Fist ', 'Open palm ', 'Peace ', 'Thumb up ']
    suffix = [' - Graph', ' - LIVE Graph', ' - ML Graph']
    plt.style.use('fivethirtyeight')
    for i in range(0, user_pref[1]):
        current_sensor = 'Sensor' + str(i + 1)
        sample = list(range(1, len(data[i]) + 1))
        voltage = [x * user_pref[3] for x in data[i]]
        plt.plot(sample, voltage, label=current_sensor)
    plt.xlabel("Sample [#]")
    plt.ylabel("Voltage [V]")
    plt.title(str(pose[user_pref[5]]) + "Voltage as a function of sample")
    plt.legend()
    t = str(time.asctime())
    file_name = str(user_pref[5]) + " " + str(pose[user_pref[5]]) + str(re.sub('[:!@#$]', '_', t))\
                + suffix[suffix_num] + '.jpeg'
    plt.savefig(file_name, bbox_inches="tight", pad_inches=1, quality=100, optimize=True)
    plt.close()


def file_reset(file_name, what_to_reset_to):
    # Turns a file into an empty list
    file_delete(file_name)
    data = what_to_reset_to
    with open(file_name, "wb") as f:
        pickle.dump(data, f)
        del data


def shuffle_in_unison(a, b):
    assert len(a) == len(b)
    shuffled_a = np.empty(a.shape, dtype=a.dtype)
    shuffled_b = np.empty(b.shape, dtype=b.dtype)
    permutation = np.random.permutation(len(a))
    for old_index, new_index in enumerate(permutation):
        shuffled_a[new_index] = a[old_index]
        shuffled_b[new_index] = b[old_index]
    return shuffled_a, shuffled_b


def ml_module(data):
    global scaler
    X = np.array(data[0])
    y = np.array(data[1])

    # Plot data
    for x, C in zip(X, y):
        if C == 0:
            plt.plot(x[0], x[1], '.r')
        elif C == 1:
            plt.plot(x[0], x[1], '.g')
        elif C == 2:
            plt.plot(x[0], x[1], '.b')
        elif C == 3:
            plt.plot(x[0], x[1], '.m')
        elif C == 4:
            plt.plot(x[0], x[1], '.k')
    #    plt.show()

    ##########################################

    # Two option to normalize data
    noRm = 1
    if noRm:
        # Normalize with mean and std
        scaler = StandardScaler()
        scaler.fit(X)
        X = scaler.transform(X)  # X = X*x_std + x_mean # Denormalize or use scaler.inverse_transform(X)
        x_mean = scaler.mean_
        x_std = scaler.scale_
    else:
        # Normalize with min and max
        x_max = np.max(X, 0)
        x_min = np.min(X, 0)
        X = (X - x_min) / (x_max - x_min)
    ##########################################

    inputs, labels = shuffle_in_unison(X, y)
    X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.15, random_state=42)

    names = ['Nearest Neighbors', 'Linear SVM', 'RBF SVM',  # 'Gaussian Process',
             'Decision Tree', 'Random Forest', 'Neural Net', 'AdaBoost']

    classifiers = [
        KNeighborsClassifier(3),
        SVC(kernel="linear", C=0.025, probability=True),
        SVC(gamma=2, C=1, probability=True),
        # GaussianProcessClassifier(1.0 * RBF(1.0)),
        DecisionTreeClassifier(max_depth=5),
        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
        MLPClassifier(alpha=1, max_iter=1000),
        AdaBoostClassifier()
    ]

    # iterate over classifiers
    scores = []
    clf_list = []
    for name, clf in zip(names, classifiers):
        clf.fit(list(X_train), list(y_train))

        score = clf.score(X_test, y_test)  # Evaluate on test data
        scores.append(score)
        print(name, score)
        clf_list.append(clf)
    scores = dict(zip(names, scores))
    return [clf_list, names, classifiers, scaler, scores]


def data_setup_for_ml_module(file_names_list):
    X = []
    y = []
    for i in range(len(file_names_list)):
        data = file_opener(file_names_list[i])
        for j in range(len(data)):
            X.append(data[j])
            y.append(i)
    return [X, y]


def prediction(data, names, classifiers, scaler, pose_num):
    noRm = 1
    X = np.array(data)
    resaults = []
    for name, clf in zip(names, classifiers):
        # print("-------------------------")
        # print('Classifier ' + name + ': ')
        for i in range(len(data)):
            y_real = pose_num
            x = X
            if noRm:
                x = scaler.transform(x.reshape(1, -1))
            else:
                x = (x - x_min) / (x_max - x_min)

            y_predict = clf.predict(x.reshape(1, -1))[0]
            dist = clf.predict_proba(x.reshape(1, -1))[0]

        # print('Real class: ' + str(y_real) + '\nPredicted class: \n' + str(prediction) + '\n' +
        #     ', distribution: \n' + str(dist))
        resaults.append(y_predict)

    return resaults


def most_common_in_a_list(L):
    SL = sorted((x, i) for i, x in enumerate(L))
    groups = itertools.groupby(SL, key=operator.itemgetter(0))

    def _auxfun(g):
        item, iterable = g
        count = 0
        min_index = len(L)
        for _, where in iterable:
            count += 1
            min_index = min(min_index, where)
        return count, -min_index

    return max(groups, key=_auxfun)[0]


def list_statistics(a_list):
    mean = statistics.mean(a_list)
    std = statistics.stdev(a_list)
    return [mean, std]
